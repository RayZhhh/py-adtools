# Code Evaluation

The `adtools.evaluator` module provides a flexible and secure framework for evaluating Python code for functionality and performance. It serves as a higher-level abstraction over `adtools.sandbox`, designed to simplify the evaluation workflow.

<h2>Evaluator Types</h2>

`adtools` offers two evaluator implementations that share a common interface, making them easily interchangeable based on your needs:

1.  **`PyEvaluator`**:
    *   Uses the standard `multiprocessing` and `shared_memory` libraries for evaluation.
    *   Avoids `pickle` serialization overhead, making it ideal for high-performance scenarios that return large objects (e.g., tensors in machine learning).

2.  **`PyEvaluatorRay`**:
    *   Leverages the `Ray` framework for distributed, secure evaluation.
    *   Supports zero-copy return of large objects.
    *   The perfect choice for cluster environments or when maximum isolation is required.

<h2>`PyEvaluator` Usage Example</h2>

The core of code evaluation is to create a subclass of `PyEvaluator` and implement its `evaluate_program` abstract method.

<h3>1. Create a Custom Evaluator</h3>

The example below defines a `SortAlgorithmEvaluator` for assessing sorting algorithms.

```python
import time
from typing import Dict, Callable, List, Any
from adtools.evaluator import PyEvaluator

class SortAlgorithmEvaluator(PyEvaluator):
    def evaluate_program(
        self,
        program_str: str,
        callable_functions_dict: Dict[str, Callable],
        **kwargs,
    ) -> Any | None:
        """
        Evaluates a sorting algorithm program.

        Args:
            program_str (str): The raw program code text.
            callable_functions_dict (Dict[str, Callable]): A dictionary mapping function names to callable function objects.
        
        Returns:
            The execution time in seconds if the algorithm is correct; otherwise, None.
        """
        # Get the function named `merge_sort` from the dictionary
        sort_algo: Callable = callable_functions_dict.get("merge_sort")
        if not sort_algo:
            return None # Evaluation fails if the function is not found

        # Prepare test data
        input_data = [10, 2, 4, 76, 19, 29, 3, 5, 1]
        
        # Record execution time
        start = time.time()
        res = sort_algo(input_data)
        duration = time.time() - start
        
        # Validate the result
        if res == sorted(input_data):
            return duration  # Correct result, return execution time as the score
        else:
            return None  # Incorrect result
```

The `evaluate_program` method receives `program_str` (the raw code) and parsed callable objects (like `callable_functions_dict`). You can use these objects to execute your testing logic.

<h3>2. Perform Evaluation</h3>

After creating an evaluator instance, use the `secure_evaluate` method to safely execute and evaluate the code.

```python
# Assume this code is generated by an LLM
code_generated_by_llm = """
def merge_sort(arr):
    if len(arr) <= 1: return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    # ... (merge logic)
    def merge(left, right):
        result = []
        i = j = 0
        while i < len(left) and j < len(right):
            if left[i] < right[j]: result.append(left[i]); i += 1
            else: result.append(right[j]); j += 1
        result.extend(left[i:]); result.extend(right[j:])
        return result
    return merge(left, right)
"""

# Harmful code containing an infinite loop
harmful_code = """
def merge_sort(arr):
    print('This output will be redirected to /dev/null by default')
    while True:
        pass
"""

if __name__ == "__main__":
    evaluator = SortAlgorithmEvaluator()

    # Evaluate the correct code
    results = evaluator.secure_evaluate(code_generated_by_llm, timeout_seconds=10)
    print(f"Score for correct code: {results['result']:.4f}s")

    # Evaluate the harmful code, which will time out after 5 seconds
    results = evaluator.secure_evaluate(harmful_code, timeout_seconds=5)
    print(f"Score for harmful code: {results['result']}")
    print(f"Error Message: {results['error_msg']}")
```

`secure_evaluate` encapsulates the sandboxing logic, ensuring that your main application remains safe and responsive, even if the evaluated code contains errors, infinite loops, or malicious operations.
